# üï∑Ô∏è Price Monitor ‚Äì Plataforma de Monitoramento de Pre√ßos

## 1. Vis√£o Geral

O **Price Monitor** √© uma plataforma para **coleta, monitoramento e compara√ß√£o de pre√ßos** em m√∫ltiplos marketplaces brasileiros. O sistema utiliza **scraping ass√≠ncrono**, **processamento desacoplado por jobs** e uma **arquitetura orientada a servi√ßos**, garantindo escalabilidade, manutenibilidade e isolamento de responsabilidades.

A aplica√ß√£o permite que usu√°rios criem coletas de pre√ßos a partir de um termo de busca, selecionem marketplaces espec√≠ficos e acompanhem a execu√ß√£o e os resultados de cada coleta.

**URL do Projeto**  
https://scrapper.prometio.com.br

---

## 2. Funcionalidades Principais

- Criar coletas de pre√ßos a partir de um **termo de busca**
- Selecionar **um ou mais marketplaces** por coleta
- Definir **quantidade m√°xima de produtos** por marketplace
- Acompanhar o **status de execu√ß√£o** dos jobs
- Visualizar **produtos coletados** com pre√ßos normalizados

---

## 3. Arquitetura Geral

A plataforma √© dividida em **tr√™s camadas independentes**, cada uma com responsabilidades bem definidas:

```
Frontend (React)
   ‚Üì REST API
Backend (Node.js / Express)
   ‚Üì HTTP + Redis Queue
Worker (Python + RQ)
   ‚Üì
Scrapers (Mercado Livre, Magazine Luiza, Americanas, etc.)
```

### Princ√≠pios adotados

- Desacoplamento entre UI, API e scraping
- Processamento ass√≠ncrono via filas
- Jobs isolados por marketplace
- Banco de dados como fonte √∫nica da verdade

---

## 4. Frontend (React)

### 4.1 Responsabilidades

- Interface do usu√°rio
- Coleta de inputs (termo de busca, marketplaces, quantidade)
- Exibi√ß√£o de execu√ß√µes e produtos coletados
- Comunica√ß√£o **exclusiva** com o Backend via API REST

‚ö†Ô∏è O frontend **n√£o executa scraping**, **n√£o acessa o banco de dados** e **n√£o se comunica diretamente com o worker**.

---

### 4.2 Tela: Nova Coleta

Na tela **Nova Coleta**, o usu√°rio informa:

- Termo de busca
- Marketplaces desejados
- Quantidade de produtos por marketplace (1 a 10)

#### Payload enviado ao backend

```json
{
  "search_term": "notebook",
  "marketplaces": [
    { "scraper_key": "mercado_livre", "max_items": 5 },
    { "scraper_key": "magazine_luiza", "max_items": 3 }
  ]
}
```

---

### 4.3 Carregamento de Marketplaces

Os marketplaces exibidos na interface **n√£o s√£o mockados** no frontend.

Eles s√£o carregados dinamicamente atrav√©s da API:

```
GET /api/marketplaces
```

Isso garante que a UI reflita exatamente os scrapers dispon√≠veis e ativos no sistema.

---

## 5. Backend API (Node.js + Express)

### 5.1 Responsabilidades

- Autentica√ß√£o de usu√°rios (JWT)
- Valida√ß√£o de payloads
- Cria√ß√£o e persist√™ncia de jobs de scraping
- Comunica√ß√£o com o worker Python
- Exposi√ß√£o de dados para o frontend

---

### 5.2 Principais Rotas

```
POST /api/auth/login
GET  /api/marketplaces
POST /api/marketplaces
POST /api/scrape
GET  /api/jobs
GET  /api/products
```

---

### 5.3 Fluxo de Cria√ß√£o de Jobs

1. O frontend envia uma requisi√ß√£o para `POST /api/scrape`
2. O backend valida o payload
3. Para cada marketplace, um **job independente** √© criado no banco
4. O backend envia o job para o worker via Redis Queue

Fluxo resumido:

```
Backend
 ‚Üí POST /enqueue/{job_id}
 ‚Üí Redis Queue
 ‚Üí tasks.run_scraper(job_id)
```

---

## 6. Worker (Python + RQ)

O worker √© respons√°vel por executar o scraping de forma ass√≠ncrona e desacoplada do backend.

---

### 6.0.1 API

O worker exp√µe uma **API HTTP em FastAPI** respons√°vel por receber requisi√ß√µes do backend Node.js e **enfileirar jobs de scraping no Redis (RQ)**.

Essa API atua como uma camada intermedi√°ria entre o backend e o sistema de filas, garantindo desacoplamento e permitindo valida√ß√µes, versionamento e observabilidade.

**Responsabilidades da API do Worker:**

- Receber requisi√ß√µes HTTP do backend
- Validar o `job_id` recebido
- Publicar o job na fila Redis (RQ)
- Retornar confirma√ß√£o de enfileiramento


### 6.1 JobRunner

O **JobRunner** √© o orquestrador central do worker. Suas responsabilidades incluem:

- Buscar o job no banco de dados
- Atualizar o status para `running`
- Criar o scraper correto a partir do `scraper_key`
- Executar o scraping respeitando o limite de itens (`max_items`)
- Persistir os produtos coletados
- Atualizar o status final (`done` ou `failed`)

---

### 6.2 ScraperFactory

A cria√ß√£o dos scrapers √© feita atrav√©s de uma f√°brica baseada em chave:

```python
scraper_factory = {
  "mercado_livre": MercadoLivreScraper,
  "magazine_luiza": MagazineLuizaScraper,
  "americanas": AmericanasScraper
}
```

‚ö†Ô∏è Caso o `scraper_key` n√£o esteja registrado, o job √© automaticamente marcado como `failed`.

---

## 7. Scrapers

Cada scraper:

- Herda de `BaseScraper`
- Implementa o m√©todo `scrape(max_items)`
- Retorna uma lista de produtos **normalizados**

### 7.1 Formato padr√£o de produto

```json
{
  "name": "Notebook Gamer",
  "price_value": 3499.90,
  "price_text": "R$ 3.499,90",
  "product_url": "https://...",
  "image_url": "https://..."
}
```

---

## 8. Banco de Dados (MySQL)

### 8.1 Tabela: marketplaces

| Campo        | Descri√ß√£o              |
|-------------|------------------------|
| id          | Identificador          |
| name        | Nome exibido           |
| scraper_key | Chave t√©cnica          |
| is_active   | Controle de ativa√ß√£o   |

---

### 8.2 Tabela: scrape_jobs

| Campo         | Descri√ß√£o                         |
|--------------|-----------------------------------|
| id           | Identificador                     |
| user_id      | Usu√°rio                           |
| marketplace_id | Marketplace                    |
| search_term  | Termo buscado                     |
| max_items    | Limite de produtos                |
| status       | queued / running / done / failed  |
| started_at   | Data de in√≠cio                    |
| finished_at  | Data de finaliza√ß√£o               |

---

### 8.3 Tabela: products

| Campo           | Descri√ß√£o           |
|-----------------|---------------------|
| scrape_job_id   | Job relacionado     |
| marketplace_id  | Marketplace         |
| name            | Nome do produto     |
| price_value     | Valor num√©rico      |
| price_text      | Pre√ßo formatado     |
| product_url     | URL do produto      |
| image_url       | URL da imagem       |

---

## 9. Decis√µes Arquiteturais

- Jobs isolados por marketplace para maior controle e paralelismo
- Scrapers totalmente desacoplados do backend
- Worker independente e escal√°vel
- Banco de dados como fonte √∫nica da verdade
- Frontend 100% din√¢mico, baseado em dados da API


---

## 10. Considera√ß√µes Finais

Essa arquitetura permite escalar scraping, adicionar novos marketplaces com baixo acoplamento e manter a aplica√ß√£o organizada, previs√≠vel e f√°cil de evoluir.

